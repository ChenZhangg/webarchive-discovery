package uk.bl.wa.hadoop.mapreduce.cdx;

import java.io.BufferedReader;
import java.io.File;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.OutputStream;
import java.net.HttpURLConnection;
import java.net.URL;
import java.util.ArrayList;
import java.util.Iterator;
import java.util.List;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import org.archive.io.ArchiveReader;
import org.archive.io.ArchiveReaderFactory;
import org.archive.io.arc.ARCReader;
import org.archive.io.warc.WARCReader;
import org.archive.wayback.core.CaptureSearchResult;
import org.archive.wayback.resourceindex.cdx.SearchResultToCDXFormatAdapter;
import org.archive.wayback.resourceindex.cdx.format.CDXFormat;
import org.archive.wayback.resourcestore.indexer.ArcIndexer;
import org.archive.wayback.resourcestore.indexer.WarcIndexer;

import uk.bl.wa.hadoop.mapreduce.lib.DereferencingArchiveToCDXRecordReader;

/**
 * 
 * This simple mapper takes the CDX lines generated by
 * 
 * @see uk.bl.wa.hadoop.mapreduce.lib.ArchiveToCDXFileInputFormat and POSTs them
 *      in chunks to an instance of the TinyCDXServer.
 * 
 * 
 * @author Andrew Jackson <Andrew.Jackson@bl.uk>
 *
 */
public class TinyCDXServerMapper extends Mapper<Text, Text, Text, Text> {

    private static final Log log = LogFactory.getLog(TinyCDXServerMapper.class);

    // The tinycdxserver URL to POST to.
    private String endpoint;

    // The batch size
    private int batch_size;

    // The batch to build up and post
    private List<Text> batch = new ArrayList<Text>();

    // Total:
    private long total_records = 0;

    /*
     * (non-Javadoc)
     * 
     * @see
     * org.apache.hadoop.mapreduce.Mapper#setup(org.apache.hadoop.mapreduce.
     * Mapper.Context)
     */
    @Override
    protected void setup(Mapper<Text, Text, Text, Text>.Context context)
            throws IOException, InterruptedException {
        super.setup(context);

        endpoint = context.getConfiguration().get("tinycdxserver.endpoint",
                "http://localhost:9090/test");
        log.warn("Sending to " + endpoint);
        batch_size = context.getConfiguration()
                .getInt("tinycdxserver.batch_size", 10000);

        System.setProperty("http.proxyHost", "explorer-private");
        System.setProperty("http.proxyPort", "3127");

    }

    /*
     * (non-Javadoc)
     * 
     * @see org.apache.hadoop.mapreduce.Mapper#map(java.lang.Object,
     * java.lang.Object, org.apache.hadoop.mapreduce.Mapper.Context)
     */
    @Override
    protected void map(Text key, Text value,
            Mapper<Text, Text, Text, Text>.Context context)
                    throws IOException, InterruptedException {
        // Add to the batch:
        batch.add(value);
        total_records++;

        // Send if we're ready:
        if (batch.size() >= batch_size) {
            log.error("Example key:" + key);
            log.error("Example value:" + value);
            send_batch(context);
        }

        // Also pass to reducers for cross-checking.
        try {
            context.write(key, value);
        } catch (Exception e) {
            log.error("Write failed.", e);
        }

    }

    private void send_batch(Context context) {
        boolean retry = true;
        while (retry) {
            try {
                // POST to the endpoint:
                URL u = new URL(endpoint);
                HttpURLConnection conn = (HttpURLConnection) u.openConnection();
                conn.setDoOutput(true);
                conn.setRequestMethod("POST");
                conn.setRequestProperty("Content-Type", "application/x-www-form-urlencoded");
                OutputStream os = conn.getOutputStream();
                for (Text t : this.batch) {
                    String line = t.toString() + "\n";
                    os.write(line.getBytes("UTF-8"));
                }
                os.close();
                if (conn.getResponseCode() == 200) {
                    // Read the response:
                    BufferedReader in = new BufferedReader(
                            new InputStreamReader(conn.getInputStream()));
                    String inputLine;
                    StringBuffer response = new StringBuffer();
                    while ((inputLine = in.readLine()) != null) {
                        response.append(inputLine);
                    }
                    in.close();
                    log.info(response.toString());
                    // It worked! No need to retry:
                    log.info("Sent " + batch.size() + " records.");
                    batch.clear();
                    retry = false;
                } else {
                    log.warn("Got response code: " + conn.getResponseCode());
                }
            } catch (Exception e) {
                log.warn("POSTing failed with ", e);
            }
            // Record progress:
            if (context != null) {
                try {
                    context.write(new Text("BATCH"), new Text("BATCH"));
                } catch (Exception e) {
                    log.error("Write failed.", e);
                }
                context.setStatus("POSTed " + total_records + " records...");
            }
        }
    }

    /*
     * (non-Javadoc)
     * 
     * @see
     * org.apache.hadoop.mapreduce.Mapper#cleanup(org.apache.hadoop.mapreduce.
     * Mapper.Context)
     */
    @Override
    protected void cleanup(Mapper<Text, Text, Text, Text>.Context context)
            throws IOException, InterruptedException {
        super.cleanup(context);
        if (this.batch.size() > 0) {
            this.send_batch(context);
        }
    }

    /**
     * 
     * @param args
     * @throws Exception
     */
    public static void main(String[] args) throws Exception {

        File inputFile = new File(args[0]);
        ArchiveReader arcreader = ArchiveReaderFactory.get(inputFile);
        arcreader.setStrict(false);
        WarcIndexer warcIndexer = new WarcIndexer();
        ArcIndexer arcIndexer = new ArcIndexer();
        Iterator<CaptureSearchResult> archiveIterator;
        if (inputFile.getName().matches("^.+\\.warc(\\.gz)?$")) {
            archiveIterator = warcIndexer.iterator((WARCReader) arcreader);
        } else {
            archiveIterator = arcIndexer.iterator((ARCReader) arcreader);
        }
        Iterator<String> cdxlines = SearchResultToCDXFormatAdapter.adapt(
                archiveIterator,
                new CDXFormat(DereferencingArchiveToCDXRecordReader.CDX_11));

        // Test it:
        TinyCDXServerMapper mapper = new TinyCDXServerMapper();
        mapper.batch_size = 2;
        mapper.endpoint = "http://localhost:9090/t2";
        while (cdxlines.hasNext()) {
            Text cdxline = new Text(cdxlines.next());
            mapper.map(cdxline, cdxline, null);
        }
        mapper.cleanup(null);

    }
}
